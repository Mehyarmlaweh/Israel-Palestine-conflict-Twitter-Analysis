{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601c7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab06a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"doc_vectorsLabeled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b553ad",
   "metadata": {},
   "source": [
    "## Extracting the Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f3f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = df.head(2001)\n",
    "X = df_labeled.iloc[:, 1:-1]  # Features\n",
    "y = df_labeled['support']  # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3ffa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of       Unnamed: 0     dim_0     dim_1     dim_2     dim_3     dim_4     dim_5  \\\n",
       "0              0 -0.057990  0.055985  0.002570  0.025762  0.052390 -0.010970   \n",
       "1              1 -0.034268 -0.048958 -0.065448 -0.067160 -0.062757  0.045409   \n",
       "2              2 -0.089613  0.084065  0.011270  0.016617  0.072491 -0.026219   \n",
       "3              3 -0.030738  0.047998 -0.013283  0.021846  0.031084 -0.038226   \n",
       "4              4 -0.175887  0.176201  0.020011  0.047555  0.122551 -0.040481   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "1996        1996  0.041768 -0.018960  0.010593 -0.012302  0.003654 -0.002383   \n",
       "1997        1997 -0.002870 -0.009623  0.014463  0.001875 -0.010985 -0.013238   \n",
       "1998        1998 -0.009619 -0.000421  0.000033  0.006329  0.006559 -0.003828   \n",
       "1999        1999  0.075217 -0.073033 -0.014893 -0.017576 -0.016714 -0.001446   \n",
       "2000        2000  0.001953 -0.001311 -0.017945  0.004201 -0.012188  0.012024   \n",
       "\n",
       "         dim_6     dim_7     dim_8  ...    dim_91    dim_92    dim_93  \\\n",
       "0    -0.046513  0.037099 -0.040894  ...  0.007125  0.003277  0.027762   \n",
       "1    -0.056806 -0.043365  0.016724  ...  0.003829 -0.008868 -0.055331   \n",
       "2    -0.025865  0.038385 -0.022409  ... -0.012077  0.012465  0.054681   \n",
       "3     0.004800  0.032119 -0.009006  ...  0.002266  0.004316  0.015135   \n",
       "4    -0.085441  0.083944 -0.062091  ... -0.022890  0.038320  0.112547   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1996  0.000765 -0.001765  0.014429  ... -0.002654 -0.014364  0.000066   \n",
       "1997  0.029356  0.006659 -0.000605  ...  0.014385  0.006977  0.009859   \n",
       "1998 -0.008182  0.005088 -0.005044  ... -0.008517  0.009527 -0.009447   \n",
       "1999 -0.008651  0.005862  0.024140  ...  0.025205  0.018126 -0.045088   \n",
       "2000 -0.031589  0.008776  0.003245  ... -0.027959  0.009750  0.006812   \n",
       "\n",
       "        dim_94    dim_95    dim_96    dim_97    dim_98    dim_99  support  \n",
       "0     0.061996  0.108289  0.025450 -0.100729 -0.000906  0.012475      1.0  \n",
       "1    -0.034808  0.100698  0.005161  0.042054  0.064880 -0.014299      0.0  \n",
       "2     0.093645  0.121830  0.027458 -0.125875 -0.017427  0.004885      0.0  \n",
       "3     0.037993  0.006771  0.007481 -0.031486 -0.011559  0.004539      1.0  \n",
       "4     0.170995  0.257175  0.058574 -0.255907 -0.074418  0.021621      1.0  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "1996 -0.026361 -0.007298 -0.018504  0.037899  0.003997  0.011523      1.0  \n",
       "1997 -0.001527  0.001189 -0.016659 -0.010516  0.019031 -0.017467      1.0  \n",
       "1998 -0.004292  0.003992  0.009966 -0.011075 -0.002712  0.002622      1.0  \n",
       "1999 -0.017812 -0.071746 -0.038427  0.054124  0.062015 -0.000901     -1.0  \n",
       "2000 -0.008251  0.038156 -0.002365 -0.028118  0.016593 -0.004334     -1.0  \n",
       "\n",
       "[2001 rows x 102 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645fb9f",
   "metadata": {},
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db0feb",
   "metadata": {},
   "source": [
    "* Random Forest\n",
    "* SVM\n",
    "* NeuralNetworkModel\n",
    "* KNN\n",
    "* Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7260a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, X, y, test_size=0.2, random_state=42):\n",
    "        try:\n",
    "            self.model = model\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "            self.X_train, self.y_train = shuffle(self.X_train, self.y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during initialization: {e}\")\n",
    "            # Handle the exception as needed, e.g., logging, raising a specific exception, etc.\n",
    "        finally:\n",
    "            # Code that should always run after try or except\n",
    "            pass\n",
    "\n",
    "    def train_evaluate(self):\n",
    "        try:\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "            y_pred = self.model.predict(self.X_test)\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            return accuracy\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training and evaluation: {e}\")\n",
    "            return None \n",
    "\n",
    "        \n",
    "class RandomForestModel:\n",
    "    def __init__(self, n_estimators=100, random_state=42):\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "class SVMModel:\n",
    "    def __init__(self):\n",
    "        self.model = SVC()\n",
    "\n",
    "class KNNModel:\n",
    "    def __init__(self, n_neighbors=20):\n",
    "        self.model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "class NeuralNetworkModel:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def build_model(self, input_dim):\n",
    "        self.model.add(Dense(units=128, activation='relu', input_dim=input_dim))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(units=64, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(units=32, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(units=1, activation='sigmoid'))\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return self.model\n",
    "\n",
    "    def train_evaluate(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=64, validation_split=0.3):\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping])\n",
    "        y_pred_proba = self.model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        y_pred = y_pred.flatten()\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "class GaussianNBModel:\n",
    "    def __init__(self):\n",
    "        self.model = GaussianNB() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d22ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.5860349127182045\n",
      "SVM Accuracy: 0.5835411471321695\n",
      "Accuracy: 0.4763092269326683\n",
      "KNN Accuracy: 0.5785536159600998\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 11ms/step - loss: 0.8757 - accuracy: 0.4219 - val_loss: 0.7244 - val_accuracy: 0.5036\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7633 - accuracy: 0.4627 - val_loss: 0.7505 - val_accuracy: 0.4846\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7234 - accuracy: 0.4433 - val_loss: 0.7492 - val_accuracy: 0.4584\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7553 - accuracy: 0.4300 - val_loss: 0.7447 - val_accuracy: 0.4204\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7203 - accuracy: 0.4392 - val_loss: 0.7186 - val_accuracy: 0.4893\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7029 - accuracy: 0.4597 - val_loss: 0.7313 - val_accuracy: 0.5606\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7507 - accuracy: 0.4944 - val_loss: 0.7263 - val_accuracy: 0.5534\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7027 - accuracy: 0.4709 - val_loss: 0.7153 - val_accuracy: 0.5059\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.4791 - val_loss: 0.7360 - val_accuracy: 0.5273\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.4791 - val_loss: 0.7405 - val_accuracy: 0.5344\n",
      "19/19 [==============================] - 0s 894us/step\n",
      "Neural Network Accuracy: 0.4841930116472546\n",
      "Gaussian Naive Bayes Accuracy: 0.5685785536159601\n"
     ]
    }
   ],
   "source": [
    "X = np.require(X, requirements=['C'])\n",
    "# Random Forest\n",
    "rf_model = RandomForestModel()\n",
    "rf_evaluator = ModelEvaluator(rf_model.model, X, y)\n",
    "rf_accuracy = rf_evaluator.train_evaluate()\n",
    "print(f'Random Forest Accuracy: {rf_accuracy}')\n",
    "\n",
    "# SVM\n",
    "svm_model = SVMModel()\n",
    "svm_evaluator = ModelEvaluator(svm_model.model, X, y)\n",
    "svm_accuracy = svm_evaluator.train_evaluate()\n",
    "print(f'SVM Accuracy: {svm_accuracy}')\n",
    "\n",
    "# KNN\n",
    "knn_model = KNNModel()\n",
    "# Split the labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN model\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "knn_evaluator = ModelEvaluator(knn_model.model, X, y)\n",
    "knn_accuracy = knn_evaluator.train_evaluate()\n",
    "print(f'KNN Accuracy: {knn_accuracy}')\n",
    "\n",
    "# Neural Network\n",
    "nn_model = NeuralNetworkModel()\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train_nn)\n",
    "X_test_nn = scaler.transform(X_test_nn)\n",
    "nn_model.build_model(X_train_nn.shape[1])\n",
    "nn_accuracy = nn_model.train_evaluate(X_train_nn, y_train_nn, X_test_nn, y_test_nn)\n",
    "print(f'Neural Network Accuracy: {nn_accuracy}')\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNBModel()\n",
    "gnb_evaluator = ModelEvaluator(gnb_model.model, X, y)\n",
    "gnb_accuracy = gnb_evaluator.train_evaluate()\n",
    "print(f'Gaussian Naive Bayes Accuracy: {gnb_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdab4ea",
   "metadata": {},
   "source": [
    "### Using Random Forest to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369b088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Random Forest model saved to random_forest_modelTweets.joblib\n"
     ]
    }
   ],
   "source": [
    "model_filename = 'random_forest_modelTweets.joblib'\n",
    "joblib.dump(rf_model.model, model_filename)\n",
    "print(f'Trained Random Forest model saved to {model_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9afdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled = df.iloc[2001:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f05e9764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_90</th>\n",
       "      <th>dim_91</th>\n",
       "      <th>dim_92</th>\n",
       "      <th>dim_93</th>\n",
       "      <th>dim_94</th>\n",
       "      <th>dim_95</th>\n",
       "      <th>dim_96</th>\n",
       "      <th>dim_97</th>\n",
       "      <th>dim_98</th>\n",
       "      <th>dim_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.011409</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.020791</td>\n",
       "      <td>-0.007415</td>\n",
       "      <td>-0.003678</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002850</td>\n",
       "      <td>-0.007854</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>-0.031620</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.011795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.048148</td>\n",
       "      <td>0.050953</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.047589</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>-0.031907</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.020716</td>\n",
       "      <td>-0.055782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023346</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>-0.081859</td>\n",
       "      <td>-0.010699</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.005060</td>\n",
       "      <td>-0.007687</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>-0.012611</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.017494</td>\n",
       "      <td>-0.003267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.038747</td>\n",
       "      <td>-0.042332</td>\n",
       "      <td>0.011826</td>\n",
       "      <td>-0.006115</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.017220</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.019872</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>-0.007121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>-0.026658</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.021367</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>-0.034557</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.006545</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014690</td>\n",
       "      <td>-0.005522</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.030871</td>\n",
       "      <td>0.059793</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>-0.057677</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>-0.007501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10479</th>\n",
       "      <td>-0.007406</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>-0.004694</td>\n",
       "      <td>0.016298</td>\n",
       "      <td>-0.013631</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>-0.016105</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016776</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.005354</td>\n",
       "      <td>-0.003273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10480</th>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.008739</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>-0.009767</td>\n",
       "      <td>-0.009472</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>-0.022520</td>\n",
       "      <td>-0.014389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016480</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.020923</td>\n",
       "      <td>0.042457</td>\n",
       "      <td>0.055520</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>-0.043073</td>\n",
       "      <td>-0.003421</td>\n",
       "      <td>0.009873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>-0.022353</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>-0.004996</td>\n",
       "      <td>-0.009791</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>-0.013453</td>\n",
       "      <td>-0.002464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.006808</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>-0.027546</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <td>-0.071252</td>\n",
       "      <td>0.088967</td>\n",
       "      <td>0.016232</td>\n",
       "      <td>0.023419</td>\n",
       "      <td>0.073283</td>\n",
       "      <td>-0.030827</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>0.064556</td>\n",
       "      <td>-0.028185</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013867</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.059626</td>\n",
       "      <td>0.108564</td>\n",
       "      <td>0.124138</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>-0.017194</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10483</th>\n",
       "      <td>0.030129</td>\n",
       "      <td>-0.024561</td>\n",
       "      <td>-0.002653</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>-0.051246</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>-0.052293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003964</td>\n",
       "      <td>0.035338</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>-0.010413</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>-0.043376</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>-0.001670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8483 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "2001  -0.011409  0.017817  0.000373  0.002293  0.020791 -0.007415 -0.003678   \n",
       "2002  -0.048148  0.050953  0.016933  0.011151  0.047589 -0.018298 -0.031907   \n",
       "2003   0.005060 -0.007687  0.011518 -0.001099  0.006345  0.001604  0.005980   \n",
       "2004   0.038747 -0.042332  0.011826 -0.006115  0.001726  0.016052  0.011315   \n",
       "2005  -0.026658  0.032831  0.002939 -0.001465  0.021367  0.006096 -0.034557   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10479 -0.007406  0.012106  0.015260  0.013355 -0.004694  0.016298 -0.013631   \n",
       "10480 -0.019942  0.032489  0.007699  0.008739  0.022398 -0.009767 -0.009472   \n",
       "10481 -0.022353  0.001433  0.003119 -0.000876  0.005436 -0.004996 -0.009791   \n",
       "10482 -0.071252  0.088967  0.016232  0.023419  0.073283 -0.030827 -0.048934   \n",
       "10483  0.030129 -0.024561 -0.002653  0.001417  0.031749 -0.002203 -0.051246   \n",
       "\n",
       "          dim_7     dim_8     dim_9  ...    dim_90    dim_91    dim_92  \\\n",
       "2001   0.004083  0.003181 -0.015718  ... -0.002850 -0.007854  0.012894   \n",
       "2002   0.022250 -0.020716 -0.055782  ... -0.023346  0.005062  0.002659   \n",
       "2003  -0.012611  0.009210 -0.004440  ...  0.003746 -0.002790 -0.013111   \n",
       "2004  -0.016276  0.007469 -0.005949  ...  0.019898  0.026572 -0.001119   \n",
       "2005   0.010107 -0.006545 -0.000379  ... -0.014690 -0.005522  0.010513   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10479  0.002034 -0.016105  0.009280  ... -0.016776 -0.009949  0.001809   \n",
       "10480  0.022057 -0.022520 -0.014389  ... -0.016480 -0.001691  0.004511   \n",
       "10481  0.008593 -0.013453 -0.002464  ...  0.002410  0.013088 -0.000786   \n",
       "10482  0.064556 -0.028185 -0.081514  ... -0.013867  0.000046  0.027804   \n",
       "10483  0.016482 -0.007307 -0.052293  ... -0.003964  0.035338  0.009978   \n",
       "\n",
       "         dim_93    dim_94    dim_95    dim_96    dim_97    dim_98    dim_99  \n",
       "2001   0.003590  0.021470  0.015965  0.005742 -0.031620 -0.007543  0.011795  \n",
       "2002   0.027743  0.063972  0.068428  0.019125 -0.081859 -0.010699  0.003827  \n",
       "2003   0.005626  0.002465 -0.012269  0.001375 -0.000010 -0.017494 -0.003267  \n",
       "2004  -0.017220 -0.000459 -0.019872  0.002720  0.019633  0.009849 -0.007121  \n",
       "2005   0.015372  0.030871  0.059793  0.006668 -0.057677 -0.004412 -0.007501  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10479  0.009280  0.003805  0.009439  0.005481  0.002766  0.005354 -0.003273  \n",
       "10480  0.020923  0.042457  0.055520  0.010976 -0.043073 -0.003421  0.009873  \n",
       "10481  0.006808  0.010075  0.019256 -0.005208 -0.027546 -0.008511 -0.005886  \n",
       "10482  0.059626  0.108564  0.124138  0.024654 -0.147103 -0.017194  0.012069  \n",
       "10483 -0.010413  0.026936  0.014905  0.002622 -0.043376  0.040325 -0.001670  \n",
       "\n",
       "[8483 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52414b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# RANDON FOREST to predict labels for the unlabeled data\n",
    "predicted_labels = rf_model.model.predict(df_unlabeled)\n",
    "df_unlabeled['predicted_support'] = predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee895bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_91</th>\n",
       "      <th>dim_92</th>\n",
       "      <th>dim_93</th>\n",
       "      <th>dim_94</th>\n",
       "      <th>dim_95</th>\n",
       "      <th>dim_96</th>\n",
       "      <th>dim_97</th>\n",
       "      <th>dim_98</th>\n",
       "      <th>dim_99</th>\n",
       "      <th>predicted_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.011409</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.020791</td>\n",
       "      <td>-0.007415</td>\n",
       "      <td>-0.003678</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007854</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>-0.031620</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.048148</td>\n",
       "      <td>0.050953</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.047589</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>-0.031907</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.020716</td>\n",
       "      <td>-0.055782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>-0.081859</td>\n",
       "      <td>-0.010699</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.005060</td>\n",
       "      <td>-0.007687</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>-0.012611</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.017494</td>\n",
       "      <td>-0.003267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.038747</td>\n",
       "      <td>-0.042332</td>\n",
       "      <td>0.011826</td>\n",
       "      <td>-0.006115</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.017220</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.019872</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>-0.026658</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.021367</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>-0.034557</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.006545</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005522</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.015372</td>\n",
       "      <td>0.030871</td>\n",
       "      <td>0.059793</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>-0.057677</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>-0.007501</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10479</th>\n",
       "      <td>-0.007406</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>-0.004694</td>\n",
       "      <td>0.016298</td>\n",
       "      <td>-0.013631</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>-0.016105</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.005354</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10480</th>\n",
       "      <td>-0.019942</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.008739</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>-0.009767</td>\n",
       "      <td>-0.009472</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>-0.022520</td>\n",
       "      <td>-0.014389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.020923</td>\n",
       "      <td>0.042457</td>\n",
       "      <td>0.055520</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>-0.043073</td>\n",
       "      <td>-0.003421</td>\n",
       "      <td>0.009873</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>-0.022353</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>-0.004996</td>\n",
       "      <td>-0.009791</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>-0.013453</td>\n",
       "      <td>-0.002464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.006808</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>-0.027546</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005886</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <td>-0.071252</td>\n",
       "      <td>0.088967</td>\n",
       "      <td>0.016232</td>\n",
       "      <td>0.023419</td>\n",
       "      <td>0.073283</td>\n",
       "      <td>-0.030827</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>0.064556</td>\n",
       "      <td>-0.028185</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.059626</td>\n",
       "      <td>0.108564</td>\n",
       "      <td>0.124138</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>-0.017194</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10483</th>\n",
       "      <td>0.030129</td>\n",
       "      <td>-0.024561</td>\n",
       "      <td>-0.002653</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>-0.051246</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>-0.052293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035338</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>-0.010413</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>-0.043376</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8483 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "2001  -0.011409  0.017817  0.000373  0.002293  0.020791 -0.007415 -0.003678   \n",
       "2002  -0.048148  0.050953  0.016933  0.011151  0.047589 -0.018298 -0.031907   \n",
       "2003   0.005060 -0.007687  0.011518 -0.001099  0.006345  0.001604  0.005980   \n",
       "2004   0.038747 -0.042332  0.011826 -0.006115  0.001726  0.016052  0.011315   \n",
       "2005  -0.026658  0.032831  0.002939 -0.001465  0.021367  0.006096 -0.034557   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10479 -0.007406  0.012106  0.015260  0.013355 -0.004694  0.016298 -0.013631   \n",
       "10480 -0.019942  0.032489  0.007699  0.008739  0.022398 -0.009767 -0.009472   \n",
       "10481 -0.022353  0.001433  0.003119 -0.000876  0.005436 -0.004996 -0.009791   \n",
       "10482 -0.071252  0.088967  0.016232  0.023419  0.073283 -0.030827 -0.048934   \n",
       "10483  0.030129 -0.024561 -0.002653  0.001417  0.031749 -0.002203 -0.051246   \n",
       "\n",
       "          dim_7     dim_8     dim_9  ...    dim_91    dim_92    dim_93  \\\n",
       "2001   0.004083  0.003181 -0.015718  ... -0.007854  0.012894  0.003590   \n",
       "2002   0.022250 -0.020716 -0.055782  ...  0.005062  0.002659  0.027743   \n",
       "2003  -0.012611  0.009210 -0.004440  ... -0.002790 -0.013111  0.005626   \n",
       "2004  -0.016276  0.007469 -0.005949  ...  0.026572 -0.001119 -0.017220   \n",
       "2005   0.010107 -0.006545 -0.000379  ... -0.005522  0.010513  0.015372   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10479  0.002034 -0.016105  0.009280  ... -0.009949  0.001809  0.009280   \n",
       "10480  0.022057 -0.022520 -0.014389  ... -0.001691  0.004511  0.020923   \n",
       "10481  0.008593 -0.013453 -0.002464  ...  0.013088 -0.000786  0.006808   \n",
       "10482  0.064556 -0.028185 -0.081514  ...  0.000046  0.027804  0.059626   \n",
       "10483  0.016482 -0.007307 -0.052293  ...  0.035338  0.009978 -0.010413   \n",
       "\n",
       "         dim_94    dim_95    dim_96    dim_97    dim_98    dim_99  \\\n",
       "2001   0.021470  0.015965  0.005742 -0.031620 -0.007543  0.011795   \n",
       "2002   0.063972  0.068428  0.019125 -0.081859 -0.010699  0.003827   \n",
       "2003   0.002465 -0.012269  0.001375 -0.000010 -0.017494 -0.003267   \n",
       "2004  -0.000459 -0.019872  0.002720  0.019633  0.009849 -0.007121   \n",
       "2005   0.030871  0.059793  0.006668 -0.057677 -0.004412 -0.007501   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "10479  0.003805  0.009439  0.005481  0.002766  0.005354 -0.003273   \n",
       "10480  0.042457  0.055520  0.010976 -0.043073 -0.003421  0.009873   \n",
       "10481  0.010075  0.019256 -0.005208 -0.027546 -0.008511 -0.005886   \n",
       "10482  0.108564  0.124138  0.024654 -0.147103 -0.017194  0.012069   \n",
       "10483  0.026936  0.014905  0.002622 -0.043376  0.040325 -0.001670   \n",
       "\n",
       "       predicted_support  \n",
       "2001                 1.0  \n",
       "2002                 1.0  \n",
       "2003                 1.0  \n",
       "2004                 1.0  \n",
       "2005                 1.0  \n",
       "...                  ...  \n",
       "10479                1.0  \n",
       "10480                1.0  \n",
       "10481                1.0  \n",
       "10482                1.0  \n",
       "10483                1.0  \n",
       "\n",
       "[8483 rows x 101 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30540f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled.to_csv(\"Predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
